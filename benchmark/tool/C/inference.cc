/* Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserve.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. */

#include <paddle/capi.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <iostream>

class Timer {
public:
  Timer(std::string name, int iter = 1) : name_(name), iter_(iter) {
    clock_gettime(CLOCK_MONOTONIC, &tp_start);
  }
  ~Timer() {
    struct timespec tp_end;
    clock_gettime(CLOCK_MONOTONIC, &tp_end);
    float time = ((tp_end.tv_nsec - tp_start.tv_nsec) / 1000000.0f);
    time += (tp_end.tv_sec - tp_start.tv_sec) * 1000;
    time /= iter_;
    std::cout << "Time of " << name_ << " " << time << " ms." << std::endl;
  }

private:
  std::string name_;
  int iter_;
  struct timespec tp_start;
};

void read_file(const char* file, void** buf, long* size) {
  FILE* fp = fopen(file, "r");
  if (fp) {
    if (fseek(fp, 0L, SEEK_END) == 0) {
      *size = ftell(fp);
      fseek(fp, 0L, SEEK_SET);
      *buf = malloc(*size);
      fread(*buf, 1, *size, fp);
    }
    fclose(fp);
  } else {
    std::cerr << "Error: cannot open " << file << " ." << std::endl;
  }
}

const char* error_string(paddle_error err) {
  switch (err) {
    case kPD_NULLPTR:
      return "nullptr error";
    case kPD_OUT_OF_RANGE:
      return "out of range error";
    case kPD_PROTOBUF_ERROR:
      return "protobuf error";
    case kPD_NOT_SUPPORTED:
      return "not supported error";
    case kPD_UNDEFINED_ERROR:
      return "undefined error";
    default:
      return "";
  }
}

#define PD_CHECK(stmt)                                          \
  do {                                                          \
    paddle_error __err__ = stmt;                                \
    if (__err__ != kPD_NO_ERROR) {                              \
      const char* str = error_string(__err__);                  \
      std::cerr << str << "(" << __err__ << ") in " #stmt "\n"; \
      exit(__err__);                                            \
    }                                                           \
  } while (0)

void init_paddle() {
  Timer time("init paddle");
  PD_CHECK(paddle_init(0, NULL));
}

paddle_gradient_machine init(std::string merged_model_path,
                             std::string config_path,
                             std::string params_dir) {
  paddle_gradient_machine machine = nullptr;
  if (!merged_model_path.empty()) {
    Timer time("create gradient_machine from merged model");
    long size = 0;
    void* merged_model = NULL;
    { read_file(merged_model_path.c_str(), &merged_model, &size); }
    PD_CHECK(paddle_gradient_machine_create_for_inference_with_parameters(
        &machine, merged_model, size));
    free(merged_model);
  } else {
    // Reading config binary file. It is generated by `convert_protobin.sh`
    if (config_path.empty()) {
      std::cerr << "Both merged_model and predict_config are null. "
                << "Please specify one of them." << std::endl;
      exit(-1);
    }
    {
      long size = 0;
      void* config = NULL;
      Timer time("create gradient_machine with model config");
      read_file(config_path.c_str(), &config, &size);
      PD_CHECK(paddle_gradient_machine_create_for_inference(
          &machine, config, (int)size));
      free(config);
    }

    if (params_dir.empty()) {
      Timer time("randomize parameters");
      PD_CHECK(paddle_gradient_machine_randomize_param(machine));
    } else {
      Timer time("load parameters from disk");
      PD_CHECK(paddle_gradient_machine_load_parameter_from_disk(
          machine, params_dir.c_str()));
    }
  }

  return machine;
}

void infer(paddle_gradient_machine machine, int input_size) {
  paddle_arguments in_args = paddle_arguments_create_none();
  PD_CHECK(paddle_arguments_resize(in_args, 1));

  // Create input matrix.
  paddle_matrix mat = paddle_matrix_create(/* sample_num */ 1,
                                           /* size */ input_size,
                                           /* useGPU */ false);
  PD_CHECK(paddle_arguments_set_value(in_args, 0, mat));

  // Get First row.
  paddle_real* array;
  PD_CHECK(paddle_matrix_get_row(mat, 0, &array));

  srand(time(0));
  for (int i = 0; i < input_size; ++i) {
    array[i] = rand() / ((float)RAND_MAX);
  }

  paddle_arguments out_args = paddle_arguments_create_none();

  PD_CHECK(paddle_gradient_machine_forward(machine,
                                           in_args,
                                           out_args,
                                           /* isTrain */ false));

  {
    Timer time("forward time", 20);
    for (int i = 0; i < 20; i++) {
      paddle_gradient_machine_forward(machine,
                                      in_args,
                                      out_args,
                                      /* isTrain */ false);
    }
  }

  paddle_arguments_destroy(out_args);
  paddle_matrix_destroy(mat);
  paddle_arguments_destroy(in_args);
}

void release(paddle_gradient_machine machine) {
  PD_CHECK(paddle_gradient_machine_destroy(machine));
}

int main(int argc, char* argv[]) {
  // parse command line arguments
  std::string merged_model_path;
  std::string config_path;
  std::string params_dir;
  int input_size;
  for (int i = 1; i < argc; ++i) {
    if (std::string(argv[i]) == "--merged_model") {
      merged_model_path = std::string(argv[++i]);
    } else if (std::string(argv[i]) == "--config_path") {
      config_path = std::string(argv[++i]);
    } else if (std::string(argv[i]) == "--params_dir") {
      params_dir = std::string(argv[++i]);
    } else if (std::string(argv[i]) == "--input_size") {
      input_size = atoi(argv[++i]);
    }
  }

  // Initialize the environment of PaddlePaddle.
  init_paddle();

  // Create a gradient machine for inference.
  paddle_gradient_machine machine =
      init(merged_model_path, config_path, params_dir);

  // Do inference with random input.
  infer(machine, input_size);

  // Release the resource.
  release(machine);

  return 0;
}
